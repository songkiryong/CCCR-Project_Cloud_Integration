# -*- coding: utf-8 -*-
"""sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rRB14E8KEqVcKzRTxYP3-upForFJ-a5l
"""

# Commented out IPython magic to ensure Python compatibility.
#깃허브에 있는 yolov5 프로젝트를 클론
# %cd /content
#!git clone https://github.com/ultralytics/yolov5.git

# Commented out IPython magic to ensure Python compatibility.
#Python package에 필요한 다른 라이브러리 설치
# %cd /content/yolov5/
#!pip install -r requirements.txt

#밑에서 나온 딥러닝 결과가 담긴 텍스트파일에서 모양정보를 파싱
def fileread(data):
  x=''
  with open(data, 'r') as file:
    text = file.read()
    if 'oblong' in text :
      x = 'oblong'
    if 'oval' in text :
      x = 'oval'
    if 'round' in text :
      x = 'round'

  return x

def result(shape, color, text_final_result):
  #모양
  sha=''
  if shape == 'round':
    sha = 1
  elif shape == 'oval':
    sha = 2
  elif shape == 'oblong':
    sha = 7

  #색
  col=''
  if color == 'white':
    col = 16384
  elif color == 'red':
    col = 64
  elif color == 'orange':
    col = 512
  elif color == 'green':
    col = 2048
  elif color == 'blue':
    col = 8196
  
  #문자

  txt = text_final_result

  result = {'sha' : sha, 'col' : col, 'ide': txt }

  return result

#문자열 인식하는 pytesseract 설치
#!sudo apt install tesseract-ocr

#pip install pytesseract

from IPython.display import Image
import os
import numpy as np
import torch
from PIL import Image
from matplotlib import pyplot as plt
import tensorflow as tf
from matplotlib import pyplot as plt
import cv2
import pytesseract

#웨이트파일과, 디텍트할 이미지를 넣어 yolov5 실행
# --weights /content/best.pt    : 웨이트파일 업로드
# --source "/content/test_data/nel.jpg"    : 디텍트할 이미지 업로드
# > msg.txt   : 실행결과를 msg.txt 로 저장 
#!python detect.py --weights /content/best_s.pt --img 416 --conf 0.5 --source "/content/test_data/xx.png" > msg.txt

def detectPill(expNum):
  shape = fileread('/content/yolov5/msg.txt')

  #torch를 import하고 torch.hub.load() 함수를 통해 미리 학습된 모델을 불러옴.
  #여기서 우리가 직접 custom한 best.pt 웨이트파일을 사용
  model = torch.hub.load('ultralytics/yolov5', 'custom', path = '/content/best_s.pt')

  #알약을 디텍트한 결과 이미지에서 바운딩박스의 각 모서리 좌표를 얻는과정

  if(expNum == -1):
    detectExpFileName = '/content/runs/detect/exp/xx.png'
  else:
    detectExpFileName = '/content/runs/detect/exp' + str(expNum) + '/xx.png'
  coordinate_img = Image.open(detectExpFileName)
  results = model(coordinate_img)

  results.xyxy[0]

  X1 = results.xyxy[0][0][0]
  Y1 = results.xyxy[0][0][1]
  X2 = results.xyxy[0][0][2]
  Y2 = results.xyxy[0][0][3]

  #float 형식으로 바꿈
  x1 = X1.item()
  y1 = Y1.item()
  x2 = X2.item()
  y2 = Y2.item()

  #얻은 좌표값으로 이미지 크롭
  cropping_area = (x1, y1, x2, y2)
  cropped_img = coordinate_img.crop(cropping_area)

  #크롭한 이미지를 저장
  fig = plt.imshow(cropped_img)
  plt.axis('off')
  plt.savefig('/content/cropped_img.jpg', bbox_inches='tight',pad_inches = 0)

  #크롭한 이미지에서 grabcut을 하는 과정
  #grabcut : 배경이 아닌 전경(객체)에 해당하는 이미지를 추출해 내는 방법

  grabcut_img = cv2.imread('/content/cropped_img.jpg')
  mask = np.zeros(grabcut_img.shape[:2], np.uint8)

  bgdModel = np.zeros((1, 65), np.float64)
  fgdModel = np.zeros((1, 65), np.float64)

  height, width, c = grabcut_img.shape

  a = int(width*0.06)
  b = int(height*0.08)
  c = int(width*0.86)
  d = int(width*0.7)
  rect = (a, b, c, d)

  cv2.grabCut(grabcut_img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)

  mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8') 
  grabcut_img = grabcut_img*mask2[:,:,np.newaxis]

  #grabcut한 이미지 저장
  cv2.imwrite('/content/grabcut_img.jpg',grabcut_img)

  filename = "/content/cropped_img.jpg".format(os.getpid())
  text = pytesseract.image_to_string(Image.open(filename), lang = 'eng')

  print(text)

  if(text == '\x0c' or text == " "):
    text_final_result = ''

  elif(text != '\x0c'):
  
    characters = "\n"
    for x in range(len(characters)):
      text_result1 = text.replace("\n",'%20')

    characters_2 = "\x0c"
    for x in range(len(characters_2)):
      text_final_result = text_result1.replace("\x0c",'')

  print(text_final_result)

  #grabcut한 이미지에서 k means clustering을 하는 과정
  #clustering : 비슷한 픽셀 값을 가지는 것들을 분류하는것

  k_means_img = cv2.imread('/content/grabcut_img.jpg')
  Z = k_means_img.reshape((-1, 3))

  Z = np.float32(Z)

  criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
  K = 2
  ret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)

  center = np.uint8(center)
  res = center[label.flatten()]
  res2 = res.reshape((k_means_img.shape))

  cv2.imwrite('/content/k_means_img.jpg',res2)

  coordinate_extract_img = Image.open('/content/k_means_img.jpg')
  coordinate_extract_img_resize = coordinate_extract_img.resize((256, 256))

  #그림의 가운데의 1px 크롭
  area = (127, 127, 128, 128)
  dot_img = coordinate_extract_img.crop(area)
  dot_img.save("/content/dot.png", "PNG")

  #크롭한 1px의 이미지에서 rgb 추출하는 과정

  color_image = Image.open('/content/dot.png')

  rgb = color_image.getpixel((0, 0))
  print(rgb)

  #rgb 색 범위
  color=''
  if (rgb[0] >= 215 and rgb[1] >= 205 and rgb[2] >= 185):
    color = 'white'
  elif (rgb[0] >= 180 and rgb[1] >= 180 and rgb[2] >= 160):
    color = 'white' 
  elif (rgb[0] >= 140 and rgb[1] <= 30 and rgb[2] <= 30):
    color = 'red' #red
  elif (rgb[0] >= 220 and rgb[1] >= 120 and rgb[2] >= 0):
    color = 'orange'
  elif (rgb[0] >= 0 and rgb[1] >=20 and rgb[2] >= 10):
    color = 'green'
  elif (rgb[0] >=180 and rgb[1] >= 200 and rgb[2] >= 200):
    color = 'blue'
  elif (rgb[0] >= 110 and rgb[1] >= 130 and rgb[2] >= 150):
    color = 'blue'
  elif (rgb[0] >= 150 and rgb[1] >= 150 and rgb[2] >= 0):
    color = 'green' #green
  elif (rgb[0] >= 0 and rgb[1] >= 0 and rgb[2] >= 50):
    color = 'blue'
  

  print(color, shape)

  return result(shape, color, text_final_result)